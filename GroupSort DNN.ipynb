{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from monotonenorm import GroupSort, direct_norm,project_norm, SigmaNet\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_execution(start,end):\n",
    "    timespan=end-start\n",
    "    minutes=timespan//60\n",
    "    secondes=timespan%60\n",
    "    heures=minutes//60\n",
    "    minutes=minutes%60\n",
    "    print(f\"{int(heures)}h {int(minutes)} min {secondes} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions/classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(data,label,proportion):\n",
    "    return data[:int(proportion*data.size(0))],label[:int(proportion*data.size(0))],data[int(proportion*data.size(0)):],label[int(proportion*data.size(0)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_achieved(labels,outputs,error,relative):\n",
    "    error_goal_achieved=0\n",
    "    abs_diff= torch.abs(labels-outputs)\n",
    "    if relative==True:\n",
    "        for i in range(len(abs_diff)):\n",
    "            if abs_diff[i]>=labels[i]*error:\n",
    "                error_goal_achieved+=1\n",
    "    else:\n",
    "        for i in range(len(abs_diff)):\n",
    "            if abs_diff[i]>=error:\n",
    "                error_goal_achieved+=1\n",
    "        \n",
    "    return error_goal_achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myData(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        self.shape=x.size(0)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index],self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_input=1\n",
    "dim_hidden_layer=8\n",
    "dim_output=1\n",
    "\n",
    "class RELUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RELUNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(dim_input,dim_hidden_layer)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(dim_hidden_layer, dim_hidden_layer)\n",
    "        self.linear3 = torch.nn.Linear(dim_hidden_layer, dim_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNet(nn.Module):\n",
    "    def __init__(self,dim_input,dim_hidden_layer,dim_output,norm,nb_hidden_layer,grouping_size):\n",
    "        \n",
    "        super(GroupNet, self).__init__()\n",
    "        self.linear1=direct_norm(torch.nn.Linear(dim_input,dim_hidden_layer ),kind=\"two-inf\")\n",
    "        self.group=GroupSort(grouping_size)\n",
    "        self.linear2=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear22=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear21=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear23=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear24=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear25=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear26=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear27=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear28=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear29=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear210=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear211=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear212=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear213=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear214=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear215=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear216=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear217=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear218=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_hidden_layer ),kind=norm)\n",
    "        self.linear3=direct_norm(torch.nn.Linear(dim_hidden_layer,dim_output ),kind=norm)\n",
    "        \n",
    "        self.nb_hidden_layer=nb_hidden_layer\n",
    "        \n",
    "\n",
    "    def forward(self, x):# l'ancienne version est n'imp a priori\n",
    "        x=self.linear1(x)\n",
    "        x=self.group(x)\n",
    "        \n",
    "        if(self.nb_hidden_layer>=2):\n",
    "            x=self.linear2(x)\n",
    "            x=self.group(x)\n",
    "            \n",
    "        if(self.nb_hidden_layer>=4):\n",
    "            x=self.linear21(x)\n",
    "            x=self.group(x)\n",
    "            x=self.linear22(x)\n",
    "            x=self.group(x)\n",
    "        \n",
    "        if(self.nb_hidden_layer>=10):\n",
    "            x=self.linear23(x)\n",
    "            x=self.group(x)\n",
    "            x=self.linear24(x)\n",
    "            x=self.group(x)\n",
    "            x=self.linear25(x)\n",
    "            x=self.group(x)\n",
    "            x=self.linear26(x)\n",
    "            x=self.group(x)\n",
    "            x=self.linear27(x)\n",
    "            x=self.group(x)\n",
    "            x=self.linear28(x)\n",
    "            x=self.group(x)\n",
    "        if(self.nb_hidden_layer==20):\n",
    "            x=self.linear29(x)\n",
    "            x=self.group(x)\n",
    "            x=self.linear210(x)\n",
    "            x=self.group(x)\n",
    "            x=self.linear211(x)\n",
    "            x=self.group(x)\n",
    "            x=self.linear212(x)\n",
    "            x=self.group(x)\n",
    "            x=self.linear213(x)\n",
    "            x=self.group(x)\n",
    "            x=self.linear214(x)\n",
    "            x=self.group(x)\n",
    "            x=self.linear215(x)\n",
    "            x=self.group(x)\n",
    "            x=self.linear216(x)\n",
    "            x=self.group(x)\n",
    "            x=self.linear217(x)\n",
    "            x=self.group(x)\n",
    "            x=self.linear218(x)\n",
    "            x=self.group(x)\n",
    "        \n",
    "        \n",
    "        x=self.linear3(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_generated=1250#attention\n",
    "proportion=0.8#attention\n",
    "batch_size=80#int(data_generated*proportion)\n",
    "#attention\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Generate data\n",
    "x= torch.tensor([[random.random()] for i in range(data_generated)])\n",
    "y_carré=(0.5*x**2)\n",
    "\n",
    "x_log=x+1\n",
    "y_log=torch.log(x_log)\n",
    "\n",
    "x_3dim= torch.tensor([[random.random(),random.random(),random.random()] for i in range(data_generated)])\n",
    "y_3dim= torch.sum((1/(2*np.sqrt(3)))*x_3dim**2,1).view(data_generated,1)\n",
    "\n",
    "x_hdim= torch.tensor(np.random.rand(data_generated,10)).float()\n",
    "y_hdim= torch.sum((1/(2*np.sqrt(10)))*x_hdim**2,1).view(data_generated,1).float()\n",
    "\n",
    "x_b=torch.tensor([random.random() for i in range(data_generated)])\n",
    "\n",
    "x_carré_b=x_b.view(x.size()[0],1)\n",
    "y_carré_b=(0.5*x_b**2 + torch.tensor([np.random.normal(loc=0,scale=0.025) for i in range(data_generated)])).view(x.size()[0],1)\n",
    "\n",
    "x_log_b=(x_b+1).view(x.size()[0],1)\n",
    "y_log_b=(torch.log(x_b+1)+ torch.tensor([np.random.normal(loc=0,scale=0.025) for i in range(data_generated)])).view(x.size()[0],1)\n",
    "\n",
    "x_3dim_b2= torch.tensor([[random.random(),random.random(),random.random()] for i in range(data_generated)])\n",
    "y_3dim_b2=(torch.sum((1/(2*np.sqrt(3)))*x_3dim_b2**2,1)+ torch.tensor([np.random.normal(loc=0,scale=0.025) for i in range(data_generated)])).view(data_generated,1) \n",
    "\n",
    "x_hdim_b= torch.tensor(np.random.rand(data_generated,10)).float()\n",
    "y_hdim_b= (torch.sum((1/(2*np.sqrt(10)))*x_hdim_b**2,1)+ torch.tensor([np.random.normal(loc=0,scale=0.025) for i in range(data_generated)])).view(data_generated,1).float()\n",
    "\n",
    "#plot\n",
    "#plt.figure(figsize=[12,8])\n",
    "#plt.scatter(x_carré_b,y_carré_b)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train,y_train,X_test,y_test=splitting(x_hdim,y_hdim,proportion)\n",
    "\n",
    "\n",
    "#Define batch length\n",
    "trainset=myData(X_train,y_train)\n",
    "testset=myData(X_test,y_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6b90b436315b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m             adam(params_with_grad,\n\u001b[0m\u001b[0;32m    158\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m     func(params,\n\u001b[0m\u001b[0;32m    214\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;31m# update step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[0mstep_t\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#params neural network\n",
    "dim_input=10\n",
    "dim_output=1\n",
    "norm=\"inf\"\n",
    "grouping_size=2 #ncore de la merde ca cest le nombre de groupes\n",
    "\n",
    "#params training\n",
    "nb_tests=10\n",
    "error_goal=0.0158\n",
    "margin_error=0\n",
    "relative=False\n",
    "epochs=1000\n",
    "lr=0.001\n",
    "set_nb_hidden_layer=[1,2,4,10,20]\n",
    "set_dim_hidden_layer=[4,8,16,32,64,128]\n",
    "\n",
    "#[1,2,4,10,20]\n",
    "#[4,8,16,32,64]\n",
    "#[40,80,160,240,400]\n",
    "#[70,140,280,420,700]\n",
    "results={}\n",
    "nb_batch=int(data_generated*proportion/batch_size)\n",
    "\n",
    "for nb_hidden_layer in set_nb_hidden_layer:\n",
    "    \n",
    "    for dim_hidden_layer in set_dim_hidden_layer:\n",
    "        \n",
    "        epochs_moyenne=[]\n",
    "        non_convergent=0\n",
    "        for w in range(nb_tests):\n",
    "\n",
    "            # Create model\n",
    "            net=GroupNet(dim_input,dim_hidden_layer,dim_output,norm,nb_hidden_layer,int(dim_hidden_layer/grouping_size))\n",
    "            #print(dim_hidden_layer/grouping_size)\n",
    "            #net=RELUNet()\n",
    "            #net = SigmaNet(GroupNet(), sigma=1.)\n",
    "            criterion =nn.MSELoss()\n",
    "            optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "            error_goal_achieved=False\n",
    "\n",
    "            for epoch in range(epochs):  \n",
    "                nb_error=0\n",
    "                running_loss = 0.0\n",
    "                \n",
    "                for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "                    inputs, labels = data\n",
    "                    val_inputs,val_labels,inputs,labels=inputs[:16],labels[:16],inputs[16:],labels[16:]\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = net(inputs)\n",
    "                    val_outputs = net(val_inputs)\n",
    "                    nb_error+=error_achieved(val_labels,val_outputs,error_goal,relative)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "                    \n",
    "                    if i %nb_batch  ==nb_batch-1:    \n",
    "                        #print(f\"[{epoch+1}, {i + 1}], loss: {running_loss / nb_batch}\")\n",
    "                        running_loss = 0.0\n",
    "                #print(nb_error)\n",
    "                if nb_error<=208*margin_error:\n",
    "                    print(str(w)+\" error goal was achieved\")\n",
    "                    error_goal_achieved=True\n",
    "                    epochs_moyenne.append(epoch)\n",
    "                    break\n",
    "\n",
    "\n",
    "            if error_goal_achieved==False:\n",
    "                print(str(w)+\" No more epochs, error goal was not achieved\")\n",
    "                epochs_moyenne.append(epochs)\n",
    "                non_convergent+=1\n",
    "               \n",
    "                if non_convergent>=3:\n",
    "                    print(\"this configuration does not seems to converge\")\n",
    "                    break\n",
    "            \n",
    "\n",
    "\n",
    "        print(f\"\\nepochs moyenne sur {w+1} tests: {sum(epochs_moyenne)/len(epochs_moyenne)}\")\n",
    "        print(epochs_moyenne)\n",
    "        print(\"\\n\\n\")\n",
    "        results[f\"{nb_hidden_layer}\\\\{dim_hidden_layer}\"]=(sum(epochs_moyenne)/len(epochs_moyenne),epochs_moyenne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "mae=0\n",
    "maxx=0\n",
    "i=0\n",
    "label_tot=np.array([])\n",
    "outputs_tot=np.array([])\n",
    "inputs_tot=np.array([])\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        total += labels.size(0)\n",
    "        abs_diff= torch.abs(outputs-labels)\n",
    "        maxx=max(abs_diff.max(),maxx)\n",
    "        mae+= abs_diff.sum()\n",
    "        label_tot=np.concatenate((label_tot,np.array(labels.squeeze())))\n",
    "        outputs_tot=np.concatenate((outputs_tot,np.array(outputs.squeeze())))\n",
    "        if i>0:\n",
    "            inputs_tot=np.concatenate((inputs_tot,np.array(inputs.squeeze())))\n",
    "        else:inputs_tot=np.array(inputs.squeeze())\n",
    "        i+=1\n",
    "        \n",
    "\n",
    "print(\"MAE: \"+ str(mae/total))\n",
    "print(\"Max: \"+ str(maxx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(inputs_tot,outputs_tot,label=\"predicted\")\n",
    "plt.scatter(inputs_tot,label_tot,label=\"true values\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
